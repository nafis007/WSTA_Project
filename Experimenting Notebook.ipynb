{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import errno\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing = \"1928_in_association_football 0 The following are the football -LRB- soccer -RRB- events of the year 1928 throughout the world .\"\n",
    "testing1 = testing.split()\n",
    "print(testing1)\n",
    "testing2 = \" \".join(testing1)\n",
    "print(testing2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing complete wiki file to pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'wiki-pages-text/*'\n",
    "files = glob.glob(path)\n",
    "count = 0\n",
    "faultySentenceIdCount = 0\n",
    "wikiShards = {}\n",
    "\n",
    "for name in files:\n",
    "    count += 1\n",
    "    with open(name, encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            eachLine = line.split()\n",
    "            docId = unicodedata.normalize('NFD', eachLine[0])\n",
    "            tempSentenceId = eachLine[1]\n",
    "            if tempSentenceId.isdigit():\n",
    "                sentenceId = int(tempSentenceId)\n",
    "                sentence = \" \".join(eachLine[2:])\n",
    "                newSentence = {sentenceId:sentence}\n",
    "                \n",
    "                if docId in wikiShards:\n",
    "                    wikiShards[docId].append(newSentence)\n",
    "                    \n",
    "                else:\n",
    "                    wikiShards[docId] = []\n",
    "                    wikiShards[docId].append(newSentence)\n",
    "            else:\n",
    "                faultySentenceIdCount += 1\n",
    "print(\"Total files : \" + count)\n",
    "print(\"Sentence without ID : \" + faultySentenceIdCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('allWikiShardsDict.pickle', 'wb') as handle:\n",
    "    pickle.dump(wikiShards, handle, protocol=pickle.HIGHEST_PROTOCOL) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "with open('allWikiShardsDict.pickle', 'rb') as handle:\n",
    "    allWikiDocs = pickle.load(handle)\n",
    "# print (wikiShards == allWikiDocs)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(allWikiDocs.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize(word):\n",
    "    lemma = lemmatizer.lemmatize(word,'v')\n",
    "    if lemma == word:\n",
    "        lemma = lemmatizer.lemmatize(word,'n')\n",
    "    return lemma\n",
    "\n",
    "\n",
    "sw = stopwords.words('english') + ['!',',','.','?','\\s',\"\\n\",\"-lrb-\",\"-rrb-\"]\n",
    "def pre_process(sentence):\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    tokens = [lemmatize(t) for t in tokens]\n",
    "    tokens = [t.lower() for t in tokens]\n",
    "    tokens =  [t for t in tokens if t not in sw]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['colin', 'kaepernick', 'become', 'start', 'quarterback', '49ers', '63rd', 'season', 'national', 'football', 'league']\n"
     ]
    }
   ],
   "source": [
    "print(pre_process(\"Colin Kaepernick became a starting quarterback during the 49ers 63rd season in the National Football League.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4411\n",
      "21050\n",
      "22548\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "all_claims_keywords = set()\n",
    "with open('devset.json', 'r') as f:\n",
    "    info = json.load(f)\n",
    "\n",
    "for key in info:\n",
    "    all_claims_keywords.update(pre_process(info[key][\"claim\"]))\n",
    "\n",
    "print(len(all_claims_keywords))\n",
    "\n",
    "\n",
    "with open('train.json', 'r') as f:\n",
    "    info = json.load(f)\n",
    "\n",
    "for key in info:\n",
    "    all_claims_keywords.update(pre_process(info[key][\"claim\"]))\n",
    "\n",
    "print(len(all_claims_keywords))\n",
    "\n",
    "with open('test-unlabelled.json', 'r') as f:\n",
    "    info = json.load(f)\n",
    "\n",
    "for key in info:\n",
    "    all_claims_keywords.update(pre_process(info[key][\"claim\"]))\n",
    "\n",
    "print(len(all_claims_keywords))\n",
    "\n",
    "with open('all_claims_keywords.pickle', 'wb') as handle:\n",
    "    pickle.dump(all_claims_keywords, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listKeys = list(allWikiDocs.keys())\n",
    "\n",
    "docId_int_dict = {}\n",
    "\n",
    "i = 0\n",
    "for key in listKeys:\n",
    "    docId_int_dict[key] = i\n",
    "    i += 1\n",
    "\n",
    "with open('docIdToIntDict.pickle', 'wb') as handle:\n",
    "    pickle.dump(docId_int_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('docIdToIntDict.pickle', 'rb') as handle:\n",
    "    docIdIntDict = pickle.load(handle)\n",
    "    \n",
    "#print(docId_int_dict == docIdIntDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('train.json', 'r') as f:\n",
    "    trainJsonDict = json.load(f)\n",
    "    \n",
    "print(len(trainJsonDict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingWikiDocs = {}\n",
    "for key in trainJsonDict.keys():\n",
    "    tempEvidenceList = trainJsonDict[key]['evidence']  \n",
    "    for evidence in tempEvidenceList:\n",
    "        foundDoc = unicodedata.normalize('NFD', evidence[0])\n",
    "        if (foundDoc in allWikiDocs) and (foundDoc not in trainingWikiDocs):\n",
    "            trainingWikiDocs[foundDoc] = allWikiDocs[foundDoc]\n",
    "            \n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('allTrainShards.pickle', 'wb') as handle:\n",
    "    pickle.dump(trainingWikiDocs, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('allTrainShards.pickle', 'rb') as handle:\n",
    "    allTrainDocs = pickle.load(handle)\n",
    "    \n",
    "#print(allTrainDocs == trainingWikiDocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for key in allTrainDocs.keys():\n",
    "    print (key,\" \",allTrainDocs[key])\n",
    "    count += 1\n",
    "    print()\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentenceList = allTrainDocs['Nikolaj_Coster-Waldau']\n",
    "#for sentence in sentenceList:\n",
    "#    rawSentence = sentence.values()\n",
    "totalSentence = []\n",
    "\n",
    "sentence = sentenceList[0]\n",
    "rawSentence = list(sentence.values())[0]\n",
    "totalSentence.append(rawSentence)\n",
    "\n",
    "\n",
    "sentence2 = sentenceList[1]\n",
    "rawSentence2 = list(sentence2.values())[0]\n",
    "totalSentence.append(rawSentence2)\n",
    "\n",
    "\n",
    "#totalSentence = rawSentence+\" \"+rawSentence2\n",
    "\n",
    "sentence3 = sentenceList[2]\n",
    "rawSentence3 = list(sentence3.values())[0]\n",
    "\n",
    "totalSentence.append(rawSentence3)\n",
    "\n",
    "finalRawDoc = \" \".join(totalSentence)\n",
    "print(finalRawDoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "docToIntMap = {}\n",
    "justDocumentList = []\n",
    "for doc in allWikiDocs.keys():\n",
    "    sentenceList = allWikiDocs[doc]\n",
    "    rawSentenceList = []\n",
    "    for sentence in sentenceList:\n",
    "        rawSentence = list(sentence.values())[0]\n",
    "        rawSentenceList.append(rawSentence)\n",
    "    finalRawDoc = \" \".join(rawSentenceList)\n",
    "    docToIntMap[doc] = i\n",
    "    justDocumentList.append(finalRawDoc)   \n",
    "    i += 1\n",
    "\n",
    "print(type(justDocumentList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('justDocumentList.pickle', 'wb') as handle:\n",
    "    pickle.dump(justDocumentList, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('docToIntMap.pickle', 'wb') as handle:\n",
    "    pickle.dump(docToIntMap, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('justDocumentList.pickle', 'rb') as handle:\n",
    "    justDocumentList = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('docToIntMap.pickle', 'rb') as handle:\n",
    "    docToIntMap = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "gen_docs = [[w.lower() for w in word_tokenize(text)] \n",
    "            for text in justDocumentList]\n",
    "#print(gen_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_docs = [[w for w in text.split()] \n",
    "            for text in justDocumentList]\n",
    "#print(gen_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(gen_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(gen_doc) for gen_doc in gen_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('corpusTemp.pickle', 'wb') as handle:\n",
    "    pickle.dump(corpus, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = gensim.models.TfidfModel(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 0\n",
    "for i in corpus:\n",
    "    s += len(i)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = gensim.similarities.Similarity('E:/(4) Semester 1 February 2019/COMP90042 Web Search and Text Analysis/Project Our/',tf_idf[corpus],\n",
    "                                      num_features=len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_doc = [w.lower() for w in word_tokenize(\"The Ten Commandments is an epic film.\")]\n",
    "#print(query_doc)\n",
    "query_doc_bow = dictionary.doc2bow(query_doc)\n",
    "#print(query_doc_bow)\n",
    "query_doc_tf_idf = tf_idf[query_doc_bow]\n",
    "#print(query_doc_tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sims)\n",
    "print(type(sims))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(sims[query_doc_tf_idf]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#mylist = [3,2,1,5,0]\n",
    "ind = np.argmax(sims[query_doc_tf_idf])\n",
    "print(ind)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(docToIntMap.keys())[list(docToIntMap.values()).index(ind)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
